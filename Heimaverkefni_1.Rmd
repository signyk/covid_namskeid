---
title: "Heimaverkefni 1"
author: "Signý Kristín Sigurjónsdóttir"
date: "6/4/2020"
output: rmdformats::readthedown
---

```{r setup, warning=F, message=F, echo=F}
library(tidyverse)
library(rmdformats)
library(grid)
library(gridExtra)
library(kableExtra)
library(rethinking)
```

# 2. kafli

Vil byrja á að biðjast afsökunar á því að sum svör eru á ensku en önnur á íslensku.

## 2E1: 
Which of the expressions below correspond to the statement: *the probability of rain on Monday?*

  (1) Pr(rain)
  
  (2) Pr(rain|Monday)
  
  (3) Pr(Monday|rain)
  
  (4) Pr(rain,Monday)/Pr(Monday)

**Lausn**: (2) og (4)

## 2E2: 
Which of the following statements corresponds to the expression: *Pr(Monday|rain)?*

  (1) The probability of rain on Monday.
  
  (2) The probability of rain, given that it is Monday.
  
  (3) The probability that it is Monday, given that it is raining.
  
  (4) The probability that it is Monday and that it is raining.
  
**Lausn**: (3)

## 2E3: 
Which of the expressions below correspond to the statement: *the probability that it is Monday, given that it is raining?*

  (1) Pr(Monday|rain)
  
  (2) Pr(rain|Monday)   
  
  (3) Pr(rain|Monday) Pr(Monday)
  
  (4) Pr(rain|Monday) Pr(Monday)/ Pr(rain)
  
  (5) Pr(Monday|rain) Pr(rain)/ Pr(Monday)

**Lausn**: (1) og (5)

## 2E4:
The Bayesian statistician Bruno de Finetti (1906–1985) began his 1973 book on probability theory with the declaration: “PROBABILITY DOES NOT EXIST.” The capitals appeared in the original, so I imagine de Finetti wanted us to shout this statement. What he meant is that probability is a device for describing uncertainty from the perspective of an observer with limited knowledge; it has no objective reality. Discuss the globe tossing example from the chapter, in light of this statement. What does it mean to say “the probability of water is 0.7”?

**Lausn**: Í þessu dæmi vitum við ekki hvert hlutfall vatns á yfirborði jarðar er. Við nýtum okkur líkur til þess að segja okkur hvert hlutfallið er, þ.e. hlutfallið er raunverulegt en líkurnar á því að lenda á vatni (eins og gert var í dæminu) eru verkfærið sem við notum til þess að lýsa hlutfallinu (sem er óþekkt) frá okkar sjónarhóli. Til þess að nálga líkurnar söfnum við gögnum.

Þannig þýðir setningin “the probability of water is 0.7” að vatn þekur 70% af yfirborði jarðar.

## 2M1: 

Recall the globe tossing model from the chapter. Compute and plot the grid approximate posterior distribution for each of the following sets of observations. In each case, assume a uniform prior for p.

  (1) W, W, W

  (2) W, W, W, L

  (3) L, W, W, L, W, W, W
  
**Lausn**:

```{r}
# define grid
p_grid <- seq( from=0 , to=1 , length.out=20 )
# define prior
prior <- rep( 1 , 20)
# compute likelihood at each value in grid 
likelihood <- dbinom( 3 , size=3 , prob=p_grid )
# compute product of likelihood and prior 
unstd.posterior <- likelihood * prior
# standardize the posterior, so it sums to 1 
posterior <- unstd.posterior / sum(unstd.posterior)

ggplot(data.frame(x=p_grid, y=posterior), aes(x, y)) + geom_line(color="dodgerblue") + geom_point(color="royalblue") + labs(title="(1)", x="probability of water", y="posterior probability")

likelihood <- dbinom(3 , size=4 , prob=p_grid )
unstd.posterior <- likelihood * prior
posterior <- unstd.posterior / sum(unstd.posterior)

ggplot(data.frame(x=p_grid, y=posterior), aes(x, y)) + geom_line(color="dodgerblue") + geom_point(color="royalblue") + labs(title="(2)", x="probability of water", y="posterior probability")

likelihood <- dbinom(5 , size=7 , prob=p_grid )
unstd.posterior <- likelihood * prior
posterior <- unstd.posterior / sum(unstd.posterior)

ggplot(data.frame(x=p_grid, y=posterior), aes(x, y)) + geom_line(color="dodgerblue") + geom_point(color="royalblue") + labs(title="(3)", x="probability of water", y="posterior probability")
```


## 2M2: 

Now assume a prior for p that is equal to zero when p < 0.5 and is a positive constant when p ≥ 0.5. Again compute and plot the grid approximate posterior distribution for each of the sets of observations in the problem just above.

**Lausn**:

```{r}
# define grid
p_grid <- seq( from=0 , to=1 , length.out=20 )
# define prior
prior <- if_else(p_grid < 0.5, 0, 1)
# compute likelihood at each value in grid 
likelihood <- dbinom( 3 , size=3 , prob=p_grid )
# compute product of likelihood and prior 
unstd.posterior <- likelihood * prior
# standardize the posterior, so it sums to 1 
posterior <- unstd.posterior / sum(unstd.posterior)

ggplot(data.frame(x=p_grid, y=posterior), aes(x, y)) + geom_line(color="indianred3") + geom_point(color="indianred4") + labs(title="(1)", x="probability of water", y="posterior probability")

likelihood <- dbinom(3 , size=4 , prob=p_grid )
unstd.posterior <- likelihood * prior
posterior <- unstd.posterior / sum(unstd.posterior)

ggplot(data.frame(x=p_grid, y=posterior), aes(x, y)) + geom_line(color="indianred3") + geom_point(color="indianred4") + labs(title="(2)", x="probability of water", y="posterior probability")

likelihood <- dbinom(5 , size=7 , prob=p_grid )
unstd.posterior <- likelihood * prior
posterior <- unstd.posterior / sum(unstd.posterior)

ggplot(data.frame(x=p_grid, y=posterior), aes(x, y)) + geom_line(color="indianred3") + geom_point(color="indianred4") + labs(title="(3)", x="probability of water", y="posterior probability")
```

## 2M4: 

Suppose you have a deck with only three cards. Each card has two sides, and each side is either black or white. One card has two black sides. The second card has one black and one white side. The third card has two white sides. Now suppose all three cards are placed in a bag and shuffled. Someone reaches into the bag and pulls out a card and places it flat on a table. A black side is shown facing up, but you don’t know the color of the side facing down. Show that the probability that the other side is also black is 2/3. Use the counting method (Section 2 of the chapter) to approach this problem. This means counting up the ways that each card could produce the observed data (a black side facing up on the table).

**Lausn**:

Að neðan sjáum við leiðinar fyrir spilin til þess að lenda með svarta hlið upp:

```{r, echo=FALSE}
tibble("Spil"=c("Spil 1", "Spil 2", "Spil 3"), "Leiðir"=c(2, 1, 0)) %>% kable() %>% kable_styling(bootstrap_options = "striped", full_width = FALSE)
```
Nú vitum við að upp kom svört hlið. Líkurnar á að *Spil 1* sé á borðinu eru 2/3 þar sem það eru tvær leiðir fyrir *Spil 1* til að vera með svörtu hliðina upp en heildarfjöldi leiða til að svarta hliðin snúi upp eru 3.

Þá eru líkurnar á að hin hliðin sé svört einnig 2/3 (jafngilt líkunum á að spilið á borðinu sé *Spil 1*)

# 3. kafli
The Easy problems use the samples from the posterior distribution for the globe tossing example.
This code will give you a specific set of samples, so that you can check your answers exactly.

```{r}
p_grid <- seq(from=0 , to=1 , length.out=1000)
prior <- rep( 1 , 1000 )
likelihood <- dbinom(6 , size=9 , prob=p_grid)
posterior <- likelihood * prior
posterior <- posterior / sum(posterior)
set.seed(100)
samples <- sample(p_grid , prob=posterior , size=1e4 , replace=TRUE)

#Mynd til að hafa til hliðsjónar
ggplot(data.frame(x=p_grid, y=posterior), aes(x, y)) + geom_line(color="dodgerblue") + geom_point(color="royalblue") + labs(x="probability of water", y="posterior probability")
```

## 3E1: 

How much posterior probability lies below p = 0.2?

**Lausn**:

```{r}
mean(samples<0.2) #telur öll TRUE og deilir með heildarfjölda athugana þ.a. það gefur líkurnar
```

## 3E2: 

How much posterior probability lies above p = 0.8?

**Laus**:

```{r}
mean(samples>0.8)
```

## 3E3: 

How much posterior probability lies between p = 0.2 and p = 0.8?

**Lausn**:

```{r}
mean(samples > 0.2 & samples < 0.8)
```
## 3E4: 

20% of the posterior probability lies below which value of p?

**Lausn**:

```{r}
quantile(samples, 0.2)
```

## 3E5: 

20% of the posterior probability lies above which value of p?

**Lausn**:

```{r}
quantile(samples, 1-0.2)
```

## 3M1: 

Suppose the globe tossing data had turned out to be 8 water in 15 tosses. Construct the posterior distribution, using grid approximation. Use the same flat prior as before.

**Lausn**:

```{r}
p_grid <- seq(from=0 , to=1 , length.out=1000)
prior <- rep( 1 , 1000 )
likelihood <- dbinom(8 , size=15 , prob=p_grid)
posterior <- likelihood * prior
posterior <- posterior / sum(posterior)

#Mynd til gamans
ggplot(data.frame(x=p_grid, y=posterior), aes(x, y)) + geom_line(color="dodgerblue") + geom_point(color="royalblue") + labs(x="probability of water", y="posterior probability")
```


## 3M2: 

Draw 10,000 samples from the grid approximation from above. Then use the samples to calculate the 90% HPDI for p.

**Lausn**:

```{r}
set.seed(100)
samples <- sample(p_grid , prob=posterior , size=1e4 , replace=TRUE)
HPDI(samples , prob=0.9) %>% t() %>% kable() %>% kable_styling(bootstrap_options = "striped", full_width = FALSE)
```

## 3M3: 

Construct a posterior predictive check for this model and data. This means simulate the distribution of samples, averaging over the posterior uncertainty in p. What is the probability of observing 8 water in 15 tosses?

**Lausn**:

```{r}
w <- rbinom(1e4 , size=15 , prob=samples)
mean(w == 8)
#mynd til gamans
ggplot(data.frame(w=as.factor(w)), aes(w)) + geom_bar(fill="cornflowerblue")
```


## 3M4: 

Using the posterior distribution constructed from the new (8/15) data, now calculate the probability of observing 6 water in 9 tosses.

**Lausn**:

```{r}
w <- rbinom(1e4 , size=9 , prob=samples)
mean(w == 6)
#mynd til gamans
ggplot(data.frame(w=as.factor(w)), aes(w)) + geom_bar(fill="cornflowerblue")
```

## 3M6: 

Suppose you want to estimate the Earth’s proportion of water very precisely. Specifically, you want the 99% percentile interval of the posterior distribution of p to be only 0.05 wide. This means the distance between the upper and lower bound of the interval should be 0.05. How many times will you have to toss the globe to do this?

**Lausn**:

```{r}
p_grid <- seq(from=0 , to=1 , length.out=1000)
prior <- rep( 1 , 1000 )
true_p=0.7

set.seed(9)

#Function to compute the width of the PI
width_PI <- function(N) {
  likelihood <- dbinom(N*true_p, size=N, prob=p_grid)
  posterior <- likelihood * prior
  posterior <- posterior / sum(posterior)
  samples <- sample(p_grid, prob=posterior, size=1e4, replace=TRUE)
  interval <- PI(samples, prob=0.99)
  diff(interval)
}

#Applies the function to a sequence of values and gets the lowest value where p<0.05
values <- seq(from=0, to=5000, by=10)
pis <- sapply(values, width_PI)
answer <- max(pis[pis<0.05]) %>% match(., pis) %>% values[.]
show(answer)
```

# 4. kafli

## 4E1: 

In the model definition below, which line is the likelihood?

$y_i ∼$ Normal($μ$, $σ$)

$μ ∼$ Normal($0, 10$)

$σ ∼$ Exponential($1$)

**Lausn**:

$y_i ∼$ Normal($μ$, $σ$)

## 4E2: 

In the model definition just above, how many parameters are in the posterior distribution?

**Lausn**:

The parameters are *unobservable*, so in the model definition above we have two parameters, $\mu$ and $\sigma$.

## 4E3: 

Using the model definition above, write down the appropriate form of Bayes’ theorem that includes the proper likelihood and priors.

**Lausn**:

$$
\text{Pr}(\mu,\sigma|y) = \frac{\prod_i \space \text{Normal}(y_i|\mu, \sigma) \space \text{Normal}(\mu|0, 10)  \space \text{Exponential}(\sigma|1)}{\int \int \prod_i \space \text{Normal}(y_i|\mu, \sigma) \space \text{Normal}(\mu|0, 10)  \space \text{Exponential}(\sigma|1)d\mu d\sigma}
$$

## 4E4: 

In the model definition below, which line is the linear model?

$y_i ∼$ Normal($μ$, $σ$)

$μ_i = α + βx_i$

$α ∼$ Normal($0$, $10$)

$β ∼$ Normal($0$, $1$)

$σ ∼$ Exponential($2$)

**Lausn**:

$μ_i = α + βx_i$ is the linear model. The first line is the likelihood and the last three lines are priors.

## 4E5: 

In the model definition just above, how many parameters are in the posterior distribution?

**Lausn**:

There are three parameters, $\alpha$, $\beta$ and $\sigma$. 

Now $\mu$ is constructed from other parameters and thus no longer a parameter to be estimated.

## 4M1: 

For the model definition below, simulate observed y values from the prior (not the posterior).

$y_i ∼$Normal($μ$, $σ$)

$μ ∼$ Normal($0$, $10$)

$σ ∼$ Exponential($1$)

**Lausn**:

```{r}
sample_mu <- rnorm(1e4, 0, 10)
sample_sigma <- rexp(1e4, 1)
prior_y <- rnorm(1e4, sample_mu, sample_sigma)
dens(prior_y)
```


## 4M2: 

Translate the model just above into a quap formula.

**Lausn**:

```{r}
flist <- alist(
  y ~ dnorm(mu, sigma), 
  mu ~ dnorm(0, 10), 
  sigma ~ dexp(1)
)
```

## 4M3: 

Translate the quap model formula below into a mathematical model definition.

```{r}
flist <- alist(
  y ~ dnorm(mu, sigma),
  mu <- a + b*x,
  a ~ dnorm(0, 10),
  b ~ dunif(0, 1),
  sigma ~ dexp(1)
)
```
**Lausn**:

$$
y_i \sim \text{Normal}(\mu_i, \sigma) \\
\mu_i = \alpha + \beta x_i \\
\alpha \sim \text{Normal}(0, 10) \\
\beta \sim \text{Uniform}(0, 1)\\
\sigma \sim \text{Exponential}(1)
$$

## 4M4: 

A sample of students is measured for height each year for 3 years. After the third year, you want to fit a linear regression predicting height using year as a predictor. Write down the mathematical model definition for this regression, using any variable names and priors you choose. Be prepared to defend your choice of priors.

**Lausn**:

Látum $h_i$ tákna hæð einstaklings $i$ og látum $x$ tákna ár.

$$
h_i \sim \text{Normal}(\mu_i, \sigma) \\
\mu_i = \alpha + \beta x_i \\
\alpha \sim \text{Normal}(175, 20) \\
\beta \sim \text{Normal}(0, 1) \\
\sigma \sim \text{Uniform}(0, 10)
$$

Fyrir $\alpha$ völdum við að láta priorinn fylgja Normaldreifingu með miðju í 175 (hæðin mín) og staðalfrávik 20. Priorinn fyrir $\beta$ fylgir einnig Normaldreifingu með miðju í 0 og staðalfrávikið 1. Þetta segir til um hversu mikið nemendur breytast í hæð á ári. Hæðin mín breytist næstum ekkert og því var þetta rökrétt.
Fyrir $\sigma$ var valin jafndreifing frá 0 til 10 þar sem nemendur eru oft mjög mismunandi í hæð.

## 4M5: 

Now suppose I remind you that every student got taller each year. Does this information lead you to change your choice of priors? How?

**Lausn**:

Þetta þýðir að $\beta$ getur ekki verið neikvætt. Því er sniðugt að láta $\beta$ fylgja Log-Normaldreifingu með sömu stika. Fáum því:

$$
h_i \sim \text{Normal}(\mu_i, \sigma) \\
\mu_i = \alpha + \beta x_i \\
\alpha \sim \text{Normal}(175, 20) \\
\beta \sim \text{Log-Normal}(0, 1) \\
\sigma \sim \text{Uniform}(0, 10)
$$

## 4M6: 

Now suppose I tell you that the variance among heights for students of the same age is never more than 64cm. How does this lead you to revise your priors?

**Lausn**:

Athugum að dreifnin er $\sigma^2$ en þá vitum við að $\sigma$ er aldrei hærra en $8$ (þar eð $8^2=64$). Ritum því:

$$
h_i \sim \text{Normal}(\mu_i, \sigma) \\
\mu_i = \alpha + \beta x_i \\
\alpha \sim \text{Normal}(175, 20) \\
\beta \sim \text{Log-Normal}(0, 1) \\
\sigma \sim \text{Uniform}(0, 8)
$$

## 4M7: 

Refit model m4.3 from the chapter, but omit the mean weight xbar this time. Compare the new model’s posterior to that of the original model. In particular, look at the covariance among the parameters. What is different? Then compare the posterior predictions of both models.

**Lausn**:

Fáum fyrst úr bókinni:

```{r}
data(Howell1)
d <- Howell1
d2 <- d[d$age >= 18, ]

xbar <- mean(d2$weight)

m4.3 <- quap(
  alist(
    height ~ dnorm(mu, sigma), 
    mu <- a+b*(weight-xbar),
    a ~ dnorm(178, 20),
    b ~ dlnorm(0, 1) ,
    sigma ~ dunif(0, 50)
  ), data=d2 )

# marginal posterior distributions of the parameters
precis(m4.3) %>% data.frame() %>% kable(col.names = c("mean", "sd", "5.5%", "94.5%")) %>% column_spec(1, bold=T) %>% kable_styling(bootstrap_options = "striped", full_width = F)

# variance-covariance matrix
vcov(m4.3) %>% round(4)

pairs(m4.3)
```

Fáum nú módelið með umbeðinni breytingu:

```{r}
set.seed(8)
m4.3_b <- quap(
  alist(
    height ~ dnorm(mu, sigma), 
    mu <- a+b*(weight),
    a ~ dnorm(178, 20),
    b ~ dlnorm(0, 1) ,
    sigma ~ dunif(0, 50)
  ), data=d2)

# marginal posterior distributions of the parameters
precis(m4.3_b) %>% data.frame() %>% kable(col.names = c("mean", "sd", "5.5%", "94.5%")) %>% column_spec(1, bold=T) %>% kable_styling(bootstrap_options = "striped", full_width = F)

# variance-covariance matrix
vcov(m4.3_b) %>% round(4)

# Correlation matrix
vcov(m4.3_b) %>% cov2cor() %>% round(4)

pairs(m4.3_b)
```

Sjáum að það er mikil fylgni milli $\alpha$ og $\beta$ í seinna módelinu.

Lítum á "posterior predictions" beggja módela:

```{r}
# Fyrra módelið
post <- extract.samples(m4.3)
a_map <- mean(post$a)
b_map <- mean(post$b)
ggplot(d2, aes(x=weight, y=height)) + geom_point(color="royalblue") + stat_function(color="indianred3", fun=function(x) a_map + b_map*(x - xbar)) + ggtitle("m4.3")

#Seinna módelið
post_b <- extract.samples(m4.3_b)
a_map_b <- mean(post_b$a)
b_map_b <- mean(post_b$b)
ggplot(d2, aes(x=weight, y=height)) + geom_point(color="royalblue") + stat_function(color="indianred3", fun=function(x) a_map_b + b_map_b*(x)) + ggtitle("m4.3_b")
```

Sjáum að það er lítill munur á því milli módela.
